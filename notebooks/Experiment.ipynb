{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder= {'node_hidden_sizes': [64], 'node_feature_dim': 1, 'edge_hidden_sizes': [4]}\n",
      "aggregator= {'node_hidden_sizes': [128], 'graph_transform_sizes': [128], 'input_size': [64], 'gated': True, 'aggregation_type': 'sum'}\n",
      "graph_embedding_net= {'node_state_dim': 64, 'edge_state_dim': 4, 'edge_hidden_sizes': [128, 128], 'node_hidden_sizes': [128], 'n_prop_layers': 5, 'share_prop_params': True, 'edge_net_init_scale': 0.1, 'node_update_type': 'gru', 'use_reverse_direction': True, 'reverse_dir_param_different': False, 'layer_norm': False, 'prop_type': 'matching'}\n",
      "graph_matching_net= {'node_state_dim': 64, 'edge_state_dim': 4, 'edge_hidden_sizes': [128, 128], 'node_hidden_sizes': [128], 'n_prop_layers': 5, 'share_prop_params': True, 'edge_net_init_scale': 0.1, 'node_update_type': 'gru', 'use_reverse_direction': True, 'reverse_dir_param_different': False, 'layer_norm': False, 'prop_type': 'matching', 'similarity': 'dotproduct'}\n",
      "model_type= matching\n",
      "data= {'problem': 'graph_edit_distance', 'dataset_params': {'n_nodes_range': [20, 20], 'p_edge_range': [0.2, 0.2], 'n_changes_positive': 1, 'n_changes_negative': 2, 'validation_dataset_size': 1000}}\n",
      "training= {'batch_size': 20, 'learning_rate': 0.0001, 'mode': 'pair', 'loss': 'cosine', 'margin': 1.0, 'graph_vec_regularizer_weight': 1e-06, 'clip_value': 10.0, 'n_training_steps': 100000, 'print_after': 100, 'eval_after': 10}\n",
      "evaluation= {'batch_size': 20}\n",
      "seed= 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphMatchingNet(\n",
       "  (_encoder): GraphEncoder(\n",
       "    (MLP1): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (MLP2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_aggregator): GraphAggregator(\n",
       "    (MLP1): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "    )\n",
       "    (MLP2): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_prop_layers): ModuleList(\n",
       "    (0-4): 5 x GraphPropMatchingLayer(\n",
       "      (_message_net): Sequential(\n",
       "        (0): Linear(in_features=132, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (_reverse_message_net): Sequential(\n",
       "        (0): Linear(in_features=132, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (GRU): GRU(192, 64)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../gmn_config/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from gmn_config.graph_utils import *\n",
    "\n",
    "from gmn_config.evaluation import compute_similarity, auc\n",
    "from gmn_config.loss import pairwise_loss, triplet_loss\n",
    "from gmn_config.gmn_utils import *\n",
    "from gmn_config.configure_cosine import *\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Print configure\n",
    "config = get_default_config()\n",
    "for k, v in config.items():\n",
    "    print(\"%s= %s\" % (k, v))\n",
    "\n",
    "# Set random seeds\n",
    "seed = config[\"seed\"]\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "training_set, validation_set = build_datasets(config)\n",
    "\n",
    "if config[\"training\"][\"mode\"] == \"pair\":\n",
    "    training_data_iter = training_set.pairs(config[\"training\"][\"batch_size\"])\n",
    "    first_batch_graphs, _ = next(training_data_iter)\n",
    "else:\n",
    "    training_data_iter = training_set.triplets(config[\"training\"][\"batch_size\"])\n",
    "    first_batch_graphs = next(training_data_iter)\n",
    "\n",
    "node_feature_dim = first_batch_graphs.node_features.shape[-1]\n",
    "edge_feature_dim = first_batch_graphs.edge_features.shape[-1]\n",
    "\n",
    "gmn, optimizer = build_model(config, node_feature_dim, edge_feature_dim)\n",
    "gmn.load_state_dict(torch.load(\"../gmn_config/model64.pth\"))\n",
    "gmn.to(device)\n",
    "gmn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GraphConv, dense_mincut_pool, GCNConv\n",
    "from torch_geometric import utils\n",
    "from torch_geometric.nn import Sequential\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../gmn_config/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.eval_metrics import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Cora'\n",
    "path = osp.join('..', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# data.edge_index, data.edge_weight = gcn_norm(  \n",
    "#                 data.edge_index, data.edge_weight, data.num_nodes,\n",
    "#                 add_self_loops=False, dtype=data.x.dtype)\n",
    "\n",
    "delta = 0.85\n",
    "edge_index, edge_weight = utils.get_laplacian(data.edge_index, data.edge_weight, normalization='sym')\n",
    "L = utils.to_dense_adj(edge_index, edge_attr=edge_weight)\n",
    "A = torch.eye(data.num_nodes) - delta*L\n",
    "data.edge_index, data.edge_weight = utils.dense_to_sparse(A)\n",
    "\n",
    "data = data.to(device)\n",
    "original_data = Data(x=reduce_dimensions(data.x.numpy()), edge_index=data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-15\n",
    "\n",
    "def just_balance_pool(x, adj, s, mask=None, normalize=True):\n",
    "    r\"\"\"The Just Balance pooling operator from the `\"Simplifying Clustering with \n",
    "    Graph Neural Networks\" <https://arxiv.org/abs/2207.08779>`_ paper\n",
    "   \n",
    "    Args:\n",
    "        x (Tensor): Node feature tensor :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times F}` \n",
    "            with batch-size :math:`B`, (maximum) number of nodes :math:`N` \n",
    "            for each graph, and feature dimension :math:`F`.\n",
    "        adj (Tensor): Symmetrically normalized adjacency tensor\n",
    "            :math:`\\mathbf{A} \\in \\mathbb{R}^{B \\times N \\times N}`.\n",
    "        s (Tensor): Assignment tensor :math:`\\mathbf{S} \\in \\mathbb{R}^{B \\times N \\times C}` \n",
    "            with number of clusters :math:`C`. The softmax does not have to be \n",
    "            applied beforehand, since it is executed within this method.\n",
    "        mask (BoolTensor, optional): Mask matrix\n",
    "            :math:`\\mathbf{M} \\in {\\{ 0, 1 \\}}^{B \\times N}` indicating\n",
    "            the valid nodes for each graph. (default: :obj:`None`)\n",
    "\n",
    "    :rtype: (:class:`Tensor`, :class:`Tensor`, :class:`Tensor`,\n",
    "        :class:`Tensor`)\n",
    "    \"\"\"\n",
    "\n",
    "    x = x.unsqueeze(0) if x.dim() == 2 else x\n",
    "    adj = adj.unsqueeze(0) if adj.dim() == 2 else adj\n",
    "    s = s.unsqueeze(0) if s.dim() == 2 else s\n",
    "\n",
    "    (batch_size, num_nodes, _), k = x.size(), s.size(-1)\n",
    "\n",
    "    s = torch.softmax(s, dim=-1)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.view(batch_size, num_nodes, 1).to(x.dtype)\n",
    "        x, s = x * mask, s * mask\n",
    "\n",
    "    out = torch.matmul(s.transpose(1, 2), x)\n",
    "    out_adj = torch.matmul(torch.matmul(s.transpose(1, 2), adj), s)\n",
    "    \n",
    "    ss = torch.matmul(s.transpose(1, 2), s)\n",
    "    ss_sqrt = torch.sqrt(ss + EPS)\n",
    "    loss = torch.mean(-_rank3_trace(ss_sqrt))\n",
    "    if normalize:\n",
    "        loss = loss / torch.sqrt(torch.tensor(num_nodes * k))\n",
    "\n",
    "    ind = torch.arange(k, device=out_adj.device)\n",
    "    out_adj[:, ind, ind] = 0\n",
    "    d = torch.einsum('ijk->ij', out_adj)\n",
    "    d = torch.sqrt(d)[:, None] + EPS\n",
    "    out_adj = (out_adj / d) / d.transpose(1, 2)\n",
    "\n",
    "    return out, out_adj, loss\n",
    "\n",
    "\n",
    "def _rank3_trace(x):\n",
    "    return torch.einsum('ijj->i', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 mp_units,\n",
    "                 mp_act,\n",
    "                 in_channels, \n",
    "                 n_clusters, \n",
    "                 mlp_units=[],\n",
    "                 mlp_act=\"Identity\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        mp_act = getattr(torch.nn, mp_act)(inplace=True)\n",
    "        mlp_act = getattr(torch.nn, mlp_act)(inplace=True)\n",
    "        \n",
    "        mp = [\n",
    "            (GCNConv(in_channels, mp_units[0], normalize=False, cached=False), 'x, edge_index, edge_weight -> x'),\n",
    "            mp_act\n",
    "        ]\n",
    "        for i in range(len(mp_units)-1):\n",
    "            mp.append((GCNConv(mp_units[i], mp_units[i+1], normalize=False, cached=False), 'x, edge_index, edge_weight -> x'))\n",
    "            mp.append(mp_act)\n",
    "        self.mp = Sequential('x, edge_index, edge_weight', mp)\n",
    "        out_chan = mp_units[-1]\n",
    "        \n",
    "        self.mlp = torch.nn.Sequential()\n",
    "        for units in mlp_units:\n",
    "            self.mlp.append(Linear(out_chan, units))\n",
    "            out_chan = units\n",
    "            self.mlp.append(mlp_act)\n",
    "        self.mlp.append(Linear(out_chan, n_clusters))\n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.mp(x, edge_index, edge_weight)\n",
    "        s = self.mlp(x)\n",
    "        adj = utils.to_dense_adj(edge_index, edge_attr=edge_weight)\n",
    "        x_pooled, adj_pooled, b_loss = just_balance_pool(x, adj, s)\n",
    "        \n",
    "        return torch.softmax(s, dim=-1), b_loss, x_pooled, adj_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "model = Net([64], \"ReLU\", dataset.num_features, dataset.num_classes, [], \"ReLU\").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss: 0.6477, NMI: 0.003, ACC: 0.299\n",
      "Epoch: 100, Loss: 0.6477, NMI: 0.003, ACC: 0.299\n",
      "Epoch: 150, Loss: 0.6477, NMI: 0.003, ACC: 0.299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eval_metrics(data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mcpu(), clust\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1001\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     23\u001b[0m         acc, nmi \u001b[38;5;241m=\u001b[39m test()\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m sim \u001b[38;5;241m=\u001b[39m similarity(gmn, config, original_data, clustered_data)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m sim\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _, loss, x_pool, adj_pool = model(data.x, data.edge_index, data.edge_weight)\n",
    "    edge_index_pool = utils.dense_to_sparse(adj_pool)[0]\n",
    "    clustered_data = torch_geometric.data.Data(x=x_pool[0], edge_index=edge_index_pool)\n",
    "    sim = similarity(gmn, config, original_data, clustered_data)\n",
    "    loss = 1 - sim\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    clust, _, _, _ = model(data.x, data.edge_index, data.edge_weight)\n",
    "    return eval_metrics(data.y.cpu(), clust.max(1)[1].cpu())\n",
    "\n",
    "for epoch in range(1, 1001):\n",
    "    train_loss = train()\n",
    "    if epoch % 50 == 0:\n",
    "        acc, nmi = test()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, NMI: {nmi:.3f}, ACC: {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0414, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "_, _, x_pool, adj_pool = model(data.x, data.edge_index, data.edge_weight)\n",
    "edge_index_pool = utils.dense_to_sparse(adj_pool)[0]\n",
    "clustered_data = torch_geometric.data.Data(x=x_pool[0], edge_index=edge_index_pool)\n",
    "sim = similarity(gmn, config, original_data, clustered_data)\n",
    "print(sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
